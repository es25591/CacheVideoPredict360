{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "57e653fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9bc6b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    from LabCacheService import CacheService\n",
    "\n",
    "cache = CacheService(capacity_bytes=2_000_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2dee1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    from LabTrajectory import simulate_viewport_with_tiles, visualize_trajectory_sequence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_frames = 1_800\n",
    "    num_tiles = 4\n",
    "\n",
    "    yaw, pitch, tiles_per_frame, tiles_array = simulate_viewport_with_tiles(\n",
    "        num_steps=n_frames,\n",
    "        n=num_tiles,\n",
    "        fov_yaw=120,\n",
    "        fov_pitch=60,\n",
    "        damping=0.99,\n",
    "        step_size=1.5,\n",
    "        start_yaw=180,\n",
    "        start_pitch=0\n",
    "    )\n",
    "    user_tiles = tiles_per_frame[:60]\n",
    "    # visualize_trajectory_sequence(\n",
    "    #     yaw, \n",
    "    #     pitch, \n",
    "    #     user_tiles, \n",
    "    #     n=num_tiles, \n",
    "    #     fov_yaw=120, \n",
    "    #     fov_pitch=60,\n",
    "    #     start_frame=0, \n",
    "    #     num_frames_to_show=60, \n",
    "    #     cols=6\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3b86e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "def start_attention_tile_prob():\n",
    "    video_path = r'C:\\Users\\es25591\\Workspace\\360dataset\\content\\saliency\\pacman_saliency.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    ret_f, f = cap.read()\n",
    "    if not ret_f:\n",
    "        return\n",
    "    att = cv2.cvtColor(f, cv2.COLOR_BGR2GRAY)    \n",
    "    \n",
    "    def split_into_tiles(arr, n):\n",
    "        h, w = arr.shape\n",
    "        tile_h, tile_w = h // n, w // n\n",
    "        tiles = []\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                y0, y1 = i * tile_h, (i + 1) * tile_h\n",
    "                x0, x1 = j * tile_w, (j + 1) * tile_w\n",
    "                tiles.append(arr[y0:y1, x0:x1])\n",
    "        return np.array(tiles)\n",
    "\n",
    "    tiles = split_into_tiles(att, n)\n",
    "\n",
    "    total_attention = np.sum(att)\n",
    "\n",
    "    att_prob = [\n",
    "        float(np.sum(tile) / total_attention) for tile in tiles\n",
    "    ]\n",
    "    \n",
    "    return att_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9f710e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPT parameters (typical values from literature; adjustable)\n",
    "alpha = 0.88  # gain curvature\n",
    "beta = 0.88   # loss curvature\n",
    "lam = 2.25    # loss aversion (losses weighted more)\n",
    "prelec_gamma = 0.65  # probability weighting parameter (Prelec)\n",
    "n = 4\n",
    "num_tiles = n*n\n",
    "class CPTPrefetchEnvSimple(gym.Env):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_tiles: int = num_tiles,\n",
    "        user_tiles: np.ndarray = user_tiles,\n",
    "        n_users: int = 1,\n",
    "        cache_capacity: int = 10,\n",
    "    ):\n",
    "        super(CPTPrefetchEnvSimple, self).__init__()\n",
    "        self.action_space = spaces.Discrete(2)  # Example: two discrete actions\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, \n",
    "            high=1, \n",
    "            shape=(4,), \n",
    "            dtype=np.float32\n",
    "        )  # Example: 4D continuous observation\n",
    "\n",
    "\n",
    "        self.capacity = self.cache_capacity = cache_capacity\n",
    "        self.sizes = np.random.randint(1, 4, size=num_tiles) \n",
    "\n",
    "        self.gain_if_prefetched = 1.0\n",
    "        self.loss_if_not_prefetched = -2.0\n",
    "\n",
    "        self.gamma = 0.7\n",
    "        self.alpha = 0.88\n",
    "        self.beta = 0.88\n",
    "        self.lam = 2.25\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.user_tiles = user_tiles\n",
    "\n",
    "        # self.current_p = np.zeros(num_tiles, dtype=float)\n",
    "        self.current_p = start_attention_tile_prob()\n",
    "        print(self.current_p)\n",
    "\n",
    "\n",
    "    def update_current_p(self) -> np.ndarray:\n",
    "        p = np.zeros(num_tiles, dtype=int)\n",
    "\n",
    "        for tile in self.user_tiles[self.step_count]:\n",
    "            tx, ty = tile[0] - 1, tile[1] - 1\n",
    "            p[tx + ty*n] = 1\n",
    "\n",
    "        return p\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.rand(4)  # Example initial state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state = np.random.rand(4)\n",
    "        reward     = 0.0\n",
    "        done       = np.random.rand() > 0.95\n",
    "        info       = {}\n",
    "\n",
    "        n_prefetch = int(action.sum())\n",
    "        info = {}\n",
    "\n",
    "        # Enforce capacity constraint and compute penalty if exceeded\n",
    "        selected = np.asarray(np.where(action == 1)[0], dtype=int)\n",
    "        if selected.size > 0 and n_prefetch > self.capacity:\n",
    "            to_drop = int(n_prefetch - self.capacity)\n",
    "            priorities = np.asarray(self.current_p)[selected]  # priorities aligned with selected\n",
    "            order = np.argsort(priorities, kind='stable')      # indices into selected, ascending priorities\n",
    "            sel_sorted = selected[order]\n",
    "            drop = sel_sorted[:to_drop]\n",
    "            action[drop] = 0\n",
    "            info[\"clipped_dropped_indices\"] = drop.tolist()\n",
    "            penalty = -0.5 * to_drop\n",
    "            reward += penalty\n",
    "\n",
    "        reward += self.compute_expected_cpt_reward(action)\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def compute_expected_cpt_reward(self, action: np.ndarray) -> float:\n",
    "        expected_reward = 0.0\n",
    "        for tile in range(num_tiles):\n",
    "            p = float(self.current_p[tile])\n",
    "            w = self.prelec_w(p)\n",
    "            if action[tile] == 1:\n",
    "                expected_outcome = p * self.gain_if_prefetched\n",
    "            else:\n",
    "                expected_outcome = p * self.loss_if_not_prefetched\n",
    "            \n",
    "            v = self.cpt_value(expected_outcome, alpha=self.alpha, beta=self.beta, lam=self.lam)\n",
    "            expected_reward += w * v\n",
    "\n",
    "        return float(expected_reward)\n",
    "\n",
    "    def u_gain(self, x):\n",
    "        return x**alpha\n",
    "\n",
    "    def u_loss(self, x):\n",
    "        return -lam * ((-x)**beta)\n",
    "    \n",
    "    def prelec_w(self, p: float, alpha: float = 0.65) -> float:\n",
    "        if p <= 0.0: return 0.0\n",
    "        if p >= 1.0: return 1.0\n",
    "        return math.exp(-((-math.log(p)) ** alpha))\n",
    "\n",
    "    def cpt_value_function(\n",
    "        self,\n",
    "        x: float, \n",
    "        alpha: float = 0.88, \n",
    "        beta: float = None, \n",
    "        lam: float = 2.25, \n",
    "        x0: float = 0.0\n",
    "    ) -> float:\n",
    "        if beta is None:\n",
    "            beta = alpha\n",
    "        diff = x - x0\n",
    "        if diff >= 0:\n",
    "            return diff ** alpha\n",
    "        else:\n",
    "            return -lam * ((-diff) ** beta)\n",
    "\n",
    "    def cpt_value(self, x: float, alpha: float = 0.88, beta: float = 0.88, lam: float = 2.25) -> float:\n",
    "        if x >= 0:\n",
    "            return x ** alpha\n",
    "        else:\n",
    "            return -lam * ((-x) ** beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4865bdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0008343041140910125, 0.02723348648278134, 0.02407229956387877, 0.0005747917525986161, 0.0006949913653000896, 0.2811378791377939, 0.1869168340207505, 0.0007377111462561423, 0.0008346325665210689, 0.23975113285164923, 0.19081337002375262, 0.000784693820453352, 0.0005634636379364598, 0.023069729968704843, 0.021688440744976792, 0.0002922388025552453]\n",
      "Obs: [0.86783995 0.30139536 0.11279525 0.67878534], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.02011653 0.57720202 0.06502369 0.99648125], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.17588261 0.54859992 0.24876559 0.74500195], Reward: -0.7447469694904795, Done: True\n",
      "Obs: [0.0936979  0.26781141 0.61545949 0.95084398], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.0908075  0.56770901 0.78860066 0.40292094], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.8986805  0.47242972 0.32114409 0.17230937], Reward: -0.7447469694904795, Done: True\n",
      "Obs: [0.54596124 0.02032327 0.43671854 0.90341438], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.25589458 0.84449422 0.48848129 0.91253194], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.51668892 0.08249435 0.09292141 0.08555096], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.77165443 0.63747709 0.74320126 0.39558671], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.34484951 0.01124316 0.44767984 0.18032057], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.54398636 0.32969293 0.66804993 0.10269653], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.71657222 0.4180673  0.23050121 0.0472613 ], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.88236791 0.18082938 0.17914656 0.31613708], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.88480039 0.09584856 0.76924562 0.22510061], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.74575659 0.34116243 0.22276299 0.04034045], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.57582533 0.04506863 0.49599081 0.70970047], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.9677112  0.11182202 0.53042184 0.98181954], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.6783994  0.33380162 0.0505056  0.35592519], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.671585   0.01740061 0.45962183 0.74342346], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.46856718 0.34874807 0.60035292 0.4461424 ], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.83742134 0.92589654 0.10397657 0.92863358], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.56350981 0.05798713 0.31214307 0.21279533], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.7610293  0.41478029 0.45109976 0.65436196], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.72289732 0.5793872  0.37941682 0.19658498], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.15164752 0.16686576 0.10610378 0.40384571], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.80193172 0.98115504 0.14994354 0.3029222 ], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.04273464 0.83675015 0.90450198 0.05355372], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.22826522 0.90343557 0.60133622 0.44033254], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.59428412 0.83010871 0.97591775 0.73188861], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.33275549 0.56463325 0.76631432 0.07376855], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.84571584 0.82410594 0.0537705  0.64758532], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.31019814 0.93906479 0.17306274 0.18664315], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.07091772 0.37233785 0.74588707 0.62195507], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.96940958 0.06361712 0.96360872 0.04417537], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.66461064 0.77392912 0.52003048 0.84881988], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.63997276 0.81851019 0.32584577 0.22979029], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.76374375 0.57472978 0.98217242 0.09471943], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.55591559 0.33848069 0.0658451  0.45192657], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.26772636 0.71036163 0.84570041 0.18529279], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.88545411 0.40161708 0.01005025 0.5671547 ], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.369741   0.00640829 0.22502693 0.88733474], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.7641445  0.28669257 0.82993858 0.95717908], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.31905304 0.35842211 0.39577613 0.35361613], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.55891569 0.46862942 0.40451267 0.47871293], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.78107675 0.4401284  0.55897654 0.3145715 ], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.92405764 0.77614308 0.58295783 0.32706117], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.7445593  0.12322775 0.5542158  0.52123712], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.90624022 0.47831566 0.91938603 0.62200831], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.71250224 0.09552114 0.86295184 0.81565896], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.64091585 0.98243219 0.99060172 0.5470423 ], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.71345092 0.02749542 0.6675965  0.71256701], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.52867751 0.15138933 0.26979731 0.91746866], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.62547289 0.45393816 0.77488004 0.79319285], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.49078295 0.8181781  0.18197605 0.83667783], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.81134373 0.07821739 0.4405658  0.28460649], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.81541639 0.2145325  0.0179542  0.04825945], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.98411814 0.5407878  0.63774611 0.20207891], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.8187217  0.73063953 0.29522671 0.79950806], Reward: -0.7447469694904795, Done: False\n",
      "Obs: [0.49561986 0.26941388 0.05023903 0.0598004 ], Reward: -0.7447469694904795, Done: False\n",
      "Total reward: -44.68481816942873\n"
     ]
    }
   ],
   "source": [
    "def map_user_tiles_to_action(step) -> np.ndarray:\n",
    "    action = np.zeros(num_tiles, dtype=int)\n",
    "    for tile in user_tiles[step]:\n",
    "        tx, ty = tile[0] - 1, tile[1] - 1\n",
    "        action[tx + ty*n] = 1\n",
    "    return action\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = CPTPrefetchEnvSimple(\n",
    "        num_tiles=num_tiles,\n",
    "        user_tiles=user_tiles,\n",
    "        n_users=1,\n",
    "        cache_capacity=50,\n",
    "    )\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for step in range(60):\n",
    "        action = map_user_tiles_to_action(step)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        print(f\"Obs: {obs}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "    print(f\"Total reward: {total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
